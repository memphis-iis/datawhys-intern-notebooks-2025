{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Copyright 2020 Dale Bowman, Andrew M. Olney and made available under [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0) for text and [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0) for code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression  \n",
    "\n",
    "In many data sets there may be several predictor variables that have an effect on a response variable.\n",
    " In fact, the *interaction* between variables may also be used to predict response.\n",
    " When we incorporate these additional predictor variables into the analysis, the model is called *multiple regression* .\n",
    " The multiple regression model builds on the simple linear regression model by adding additional predictors with corresponding parameters.\n",
    " \n",
    "## What you will learn\n",
    "\n",
    "In the sections that follow you will learn about multiple regression, an extension of simple linear regression, and how it can be used to model your data for prediction.  We will study the following:\n",
    "\n",
    "- The multiple regression model\n",
    "- Interaction effects\n",
    "- Feature selection\n",
    "- Categorical variables\n",
    "- Diagnostics\n",
    "\n",
    "## When to use multiple regression\n",
    "\n",
    "Multiple regression models are useful when you have a continuous response variable and you believe there are multiple predictors that have a linear relationship with the response variable.  The ultimate goals of fitting a multiple regression model are to 1) predict the response for a new set of features and 2) determine which predictors are most influential on the response.\n",
    "\n",
    "\n",
    "## Multiple Regression Model\n",
    "Let's suppose we are interested in determining what factors might influence a baby's birth weight.\n",
    " In our data set we have information on birth weight, our response, and predictors: mother's age, weight and height and gestation period.\n",
    " A *main effects model*  includes each of the possible predictors but no interactions.\n",
    " Suppose we name these features as in the chart below.\n",
    " \n",
    "| Variable | Type  | Description       |\n",
    "|:--------|:-------|:-------------------|\n",
    "| BW       | Ratio | baby birth weight |\n",
    "| MA       | Ratio | mother's age      |\n",
    "| MW       | Ratio | mother's weight   |\n",
    "| MH       | Ratio | mother's height   |\n",
    "| GP       | Ratio | gestation period  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the theoretical main effects multiple regression model is \n",
    "\n",
    "$$BW = \\beta_0 + \\beta_1 MA + \\beta_2 MW + \\beta_3 MH + \\beta_4 GP+ \\epsilon.$$ \n",
    "\n",
    "Now we have five parameters to estimate from the data, $\\beta_0, \\beta_1, \\beta_2, \\beta_3$ and $\\beta_4$.\n",
    " The random error term, $\\epsilon$ has the same interpretation as in simple linear regression and is assumed to come from a normal distribution with mean equal to zero and variance equal to $\\sigma^2$.\n",
    " Note that multiple regression also includes the polynomial models discussed in the simple linear regression notebook.\n",
    " \n",
    "One of the most important things to notice about the equation above is that each variable makes a contribution **independently** of the other variables.\n",
    "This is sometimes called **additivity**: the effects of predictor variable are added together to get the total effect on `BW`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Effects\n",
    "\n",
    "Suppose in the example, through exploratory data analysis, we discover that younger mothers with long gestational times tend to have heavier babies, but older mother with short gestational times tend to have lighter babies.\n",
    " This could indicate an interaction effect on the response.\n",
    " When there is an interaction effect, the effects of the variables involved are not additive.\n",
    " \n",
    " Different numbers of variables can be involved in an interaction.\n",
    " When two features are involved in the interaction it is called a *two-way interaction* .\n",
    " There are three-way and higher interactions possible as well, but they are less common in practice.\n",
    " The *full model*  includes main effects and all interactions.\n",
    " For the example given here there are 6 two-way interactions possible between the variables, 4 possible three-way, and 1 four-way interaction in the full model.\n",
    " \n",
    " Often in practice we fit the full model to check for significant interaction effects.\n",
    " If there are no interactions that are significantly different from zero, we can drop the interaction terms and fit the main effects model to see which of those effects are significant.\n",
    " If interaction effects are significant (important in predicting the behavior of the response) then we will interpret the effects of the model in terms of the interaction.\n",
    " \n",
    "<!--  NOTE: not sure if correction for multiple comparisons is outside the scope here; I would in general not recommend to students that they test all possible interactions unless they had a theoretical reason to, or unless they were doinging something exploratory and then would collect new data to test any interaction found. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Suppose we run a full model for the four variables in our example and none of the interaction terms are significant.\n",
    " We then run a main effects model and we get parameter estimates as shown in the table below.\n",
    " \n",
    "| Coefficients | Estimate | Std. Error | p-value |\n",
    "|--------------|----------|------------|---------|\n",
    "| Intercept    | 36.69    | 5.97       | 1.44e-6 |\n",
    "| MA           | 0.36     | 1.00       | 0.7197  |\n",
    "| MW           | 3.02     | 0.85       | 0.0014  |\n",
    "| MH           | -0.02    | 0.01       | 0.1792  |\n",
    "| GP           | -0.81    | 0.66       | 0.2311  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the p-value is the probability of getting the estimate that we got from the data or something more extreme (further from zero).\n",
    " Small p-values (typically less than 0.05) indicate the associated parameter is different from zero, implying that the associated covariate is important to predict response.\n",
    " In our birth weight example, we see the p-value for the intercept is very low $1.44 \\times 10^{-6}$ and so the intercept is not at zero.\n",
    " The mother's weight (`MW`) has p-value 0.0014 which is very small, indicating that mother's weight has an important (significant) impact on her baby's birth weight.\n",
    " The p-value from all other Wald tests are large: 0.7197, 0.1792, and 0.2311, so we know none of these variables are important when predicting the birth weight.\n",
    " \n",
    "  We can modify the coefficient of determination to account for having more than one predictor in the model, called the *adjusted R-square* .\n",
    " R-square has the property that as you add more terms, it will always increase.\n",
    " The adjustment for more terms takes this into consideration.\n",
    " For this data the adjusted R-square is 0.8208, indicating a reasonably good fit.\n",
    "\n",
    "  Different combinations of the variables included in the model may give better or worse fits to the data.\n",
    " We can use several methods to select the \"best\" model for the data.\n",
    " One example is called *forward selection* .\n",
    " This method begins with an empty model (intercept only) and adds variables to the model one by one until the full main effects model is reached.\n",
    " In each forward step, you add the one variable that gives the best improvement to the fit.\n",
    " There is also *backward selection*  where you start with the full model and then drop the least important variables one at a time until you are left with the intercept only.\n",
    " If there are not too many features, you can also look at all possible models.\n",
    " Typically these models are compared using the AIC (Akaike information criterion) which measures the relative quality of models.\n",
    " Given a set of models, the preferred model is the one with the minimum AIC value.\n",
    " \n",
    "Previously we talked about splitting the data into training and test sets.\n",
    "In statistics, this is not common, and the models are trained with all the data.\n",
    "This is because statistics is generally more interested in the effect of a particular variable *across the entire dataset* than it is about using that variable to make a prediction about a particular datapoint.\n",
    "Because of this, we typically have concerns about how well linear regression will work with new data, i.e. will it have the same $r^2$ for new data or a lower $r^2$?\n",
    "Both forward and backward selection potentially enhance this problem because they tune the model to the data even more closely by removing variables that aren't \"important.\"\n",
    "You should always be very careful with such variable selection methods and their implications for model generalization.\n",
    "\n",
    "<!-- NOTE: sklearn does not seem to support forward/backward https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm ; what it does support is sufficiently different/complicated that it doesn't seem useful to try to introduce it now ; this is an example where the given text would fit R perfectly but be difficult for python -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "\n",
    "In the birth weight example, there is also information available about the mother's activity level during her pregnancy.\n",
    " Values for this categorical variable are: low, moderate, and high.\n",
    " How can we incorporate these into the model? \n",
    " Since they are not numeric, we have to create *dummy variables*  that are numeric to use.\n",
    " A dummy variable represents the presence or absence of a level of the categorical variable by a 1 and the absence by a zero.\n",
    "  Fortunately, most software packages that do multiple regression do this for us automatically.\n",
    " \n",
    "Often, one of the levels of the categorical variable is considered the \"baseline\" and the contributions to the response of the other levels are in relation to baseline.\n",
    "Let's look at the data again. \n",
    " In the table below, the mother's age is dropped and the mother's activity level (MAL) is included.\n",
    " \n",
    " | Coefficients | Estimate | Std. Error | p-value  |\n",
    "|--------------|----------|------------|----------|\n",
    "| Intercept    | 31.35    | 4.65       | 3.68e-07 |\n",
    "| MW           | 2.74     | 0.82       | 0.0026   |\n",
    "| MH           | -0.04    | 0.02       | 0.0420   |\n",
    "| GP           | 1.11     | 1.03       | 0.2917   |\n",
    "| MALmoderate  | -2.97    | 1.44       | 0.049     |\n",
    "| MALhigh      | -1.45    | 2.69       | 0.5946   |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical variable `MAL`,  `MALlow` has been chosen as the baseline.\n",
    " The other two levels have parameter estimates that we can use to determine which are significantly different from the low level.\n",
    " This makes sense because all mothers will at least have low activity level, and the two additional dummy variables `MALhigh` and `MALmoderate` just get added on top of that.\n",
    " \n",
    " We can see that `MAL` moderate level is significantly different from the low level (p-value < 0.05).\n",
    " The parameter estimate for the moderate level of `MAL` is -2.97.\n",
    " This can be interpreted as: being in the moderately active group decreases birth weight by 2.97 units compared to babies in the low activity group.\n",
    " We also see that for babies with mothers in the high activity group, their birth weights are not different from birth weights in the low group, since the p-value is not low (0.5946 &gt; 0.05) and so this term does not have a significant effect on the response (birth weight).\n",
    " \n",
    "  This example highlights a phenomenon that often happens in multiple regression.\n",
    " When we drop the variable `MA` (mother's age) from the model and the categorical variable is included, both `MW` (mother's weight) and `MH` (mother's height) are both important predictors of birth weight (p-values 0.0026 and 0.0420 respectively).\n",
    " This is why it is important to perform some systematic model selection (forward or backward or all possible) to find an optimum set of features.\n",
    " \n",
    "# Diagnostics\n",
    "\n",
    "As in the simple linear regression case, we can use the residuals to check the fit of the model.\n",
    " Recall that the residuals are the observed response minus the predicted response.\n",
    " \n",
    "  - Plot the residuals against each independent variable to check whether higher order terms are needed  \n",
    "  - Plot the residuals versus the predicted values to check whether the variance is constant  \n",
    "  - Plot a qq-plot of the residuals to check for normality  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity\n",
    "\n",
    "Multicollinearity occurs when two variables or features are linearly related, i.e.\n",
    " they have very strong correlation between them (close to -1 or 1).\n",
    " Practically this means that some of the independent variables are measuring the same thing and are not needed.\n",
    " In the extreme case (close to -1 or 1), the estimates of the parameters of the model cannot be obtained.\n",
    " This is because there is no unique solution for OLS when multicolinearity occurs.\n",
    " As a result, multicollinearity makes conclusions about which features should be used questionable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Trees\n",
    "\n",
    "Let's take a look at a dataset we've seen before, `trees`, but with an additional tree type added `plum`:\n",
    "\n",
    "| Variable | Type  | Description                                           |\n",
    "|----------|-------|:-------------------------------------------------------|\n",
    "| Girth    | Ratio | Tree diameter (rather than girth, actually) in inches |\n",
    "| Height   | Ratio | Height in ft                                          |\n",
    "| Volume   | Ratio | Volume of timber in cubic ft                          |\n",
    "| Type | Nominal | The type of tree, cherry or plum |\n",
    "\n",
    "Much of what we'll do is the same as with simple linear regression, except:\n",
    "\n",
    "- Converting categorical variables into dummy variables\n",
    "- Different multiple predictors\n",
    "- Interactions\n",
    "\n",
    "### Load data\n",
    "\n",
    "Start by importing `pandas` so we can work with dataframes:\n",
    "\n",
    "- `import pandas as pd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataframe with a CSV file:\n",
    "    \n",
    "- Set  `dataframe` to with `pd` do `read_csv` using\n",
    "    - `\"datasets/trees2.csv\"`\n",
    "- `dataframe` (to display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that later on, we'd like to use `Type` as a predictor, so we need to convert it into a dummy variable.\n",
    "\n",
    "However, we'd also like to keep `Type` as a column for our plot labels. \n",
    "There are several ways to do this, but probably the easiest is to save `Type` and then put it back in the dataframe.\n",
    "\n",
    "It will make sense as we go:\n",
    "\n",
    "- Set `treeType` to `dataframe[` list containing `\"Type\"` `]` (use {dictVariable}[] from LISTS)\n",
    "- `treeType` (to display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the dummy conversion:\n",
    "    \n",
    "- Set `dataframe` to with `pd` do `get_dummies` using\n",
    "    - `dataframe`\n",
    "    - freestyle `drop_first=True`\n",
    "- `dataframe` (to display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `cherry` is now the base level, so `Type_plum` is in `0` where `cherry` was before and `1` where `plum` was before.\n",
    "\n",
    "To put `Type` back in, use `assign`:\n",
    "\n",
    "- Set `dataframe` to with `dataframe` do `assign` using\n",
    "    - freestyle `Type=treeType`\n",
    "- `dataframe` (to display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice - we have our dummy code for modeling but also the nice original label in `Type` so we don't get confused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data\n",
    "\n",
    "Let's start with some *overall* descriptive statistics:\n",
    "\n",
    "- with `dataframe` do `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice, but we suspect there might be some differences between cherry trees and plum trees that this doesn't show.\n",
    "\n",
    "We can `describe` each group as well:\n",
    "\n",
    "- Set `groups` to with `dataframe` do `groupby` using \n",
    "    - `\"Type\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `describe` groups:\n",
    "\n",
    "- with `groups` do `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this results table has been rotated compared to the normal `describe`.\n",
    "The rows are our two tree types, and the columns are **stacked columns** where the header (e.g. `Girth`) applies to the labels below it until they start repeating. \n",
    "\n",
    "From this we see that the `Girth` is about the same across trees, the `Height` is  13ft different on average, and `Volume` is 5ft different on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a plot.\n",
    "We can sneak all the variables into a 2D scatterplot with some clever annotations.\n",
    "\n",
    "First import `plotly` for plotting:\n",
    "\n",
    "- `import plotly.express as px`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the scatterplot:\n",
    "\n",
    "- with `px` do `scatter` using\n",
    "    - `dataframe`\n",
    "    - freestyle `x=\"Height\"`\n",
    "    - freestyle `y=\"Volume\"`\n",
    "    - freestyle `color=\"Type\"`\n",
    "    - freestyle `size=\"Girth\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this scatterplot, each point **position** shows the relationship between `Height` and `Volume`, each point **size** shows `Girth`, and each **color** represents these values for a different `Type`.\n",
    "It's not always possible to pack so much information into a single plot, but it can be extraordinarily useful when you can do it.\n",
    "In this case, we can see that `Girth` also increases with `Height` and `Volume`, but this relationship does not appear to change with `Type`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling 1\n",
    "\n",
    "Last time we looked at `trees`, we used `Height` to predict `Volume`.\n",
    "With multiple linear regression, we can use more that one variable.\n",
    "Let's start with using `Girth` and `Height` to predict `Volume`.\n",
    "\n",
    "Start by importing `sklearn.linear_model` for regression and `numpy` for conversion:\n",
    "\n",
    "- `import sklearn.linear_model as sklearn`\n",
    "- `import numpy as np`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the regression model:\n",
    "\n",
    "- Create variable `lm` (for linear model)\n",
    "- Set it to `with sklearn create LinearRegression using`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using all the data:\n",
    "\n",
    "- with `lm` do `fit` using\n",
    "    - `dataframe [ ]`  (use {dictVariable} from LISTS) containing a list containing\n",
    "        - `\"Girth\"` (this is $X_1$)\n",
    "        - `\"Height\"` (this is $X_2$)\n",
    "    - `dataframe [ ]` containing a list containing\n",
    "        - `\"Volume\"` (this is $Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and get the $r^2$ ; you can just copy the blocks from the last cell and change `fit` to `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that $r^2$, we'd think we have a really good model, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics 1\n",
    "\n",
    "To check the model, the first thing we need to do is get the predictions from the model. \n",
    "Once we have the predictions, we can `assign` them to a column in the `dataframe`:\n",
    "\n",
    "- Set `dataframe` to with `dataframe` do `assign` using\n",
    "    - freestyle `predictions1=` *followed by*\n",
    "        - with `lm` do `predict` using\n",
    "            - `dataframe [ ]` containing a list containing\n",
    "                - `\"Girth\"`\n",
    "                - `\"Height\"`\n",
    "- `dataframe` (to display)\n",
    "\n",
    "**This makes a very long block, so you probably want to create all the blocks and then connect them in reverse order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we want to add the residuals to `dataframe`:\n",
    "    \n",
    "- Set `dataframe` to with `dataframe` do `assign` using\n",
    "    - freestyle `residuals1=` *followed by* `dataframe [ \"Volume\" ] - dataframe [ \"predictions1\" ]`\n",
    "\n",
    "- `dataframe` (to display)\n",
    "\n",
    "**Hint: use {dictVariable}[] and the + block from MATH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some plots!\n",
    "\n",
    "Let's check linearity and equal variance:\n",
    "\n",
    "- Linearity means the residuals will be close to zero\n",
    "- Equal variance means residuals will be evenly away from zero\n",
    "\n",
    "- with `px` do `scatter` using\n",
    "    - `dataframe`\n",
    "    - freestyle `x=\"predictions1\"`\n",
    "    - freestyle `y=\"residuals1\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see something very, very wrong here: a \"U\" shape from left to right.\n",
    "This means our residuals are positive for low predictions, go negative for mid predictions, and go positive again for high predictions.\n",
    "The only way this can happen is if something is quadratic (squared) in the phenomenon we're trying to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling 2\n",
    "\n",
    "Step back for a moment and consider what we are trying to do.\n",
    "We are trying to predict volume from other measurements of the tree.\n",
    "What is the formula for volume?\n",
    "\n",
    "$$V = \\pi r^2 h$$\n",
    "\n",
    "Since this is the mathematical definition, we don't expect any differences for `plum` vs. `cherry`.\n",
    "\n",
    "What are our variables?\n",
    "\n",
    "- `Volume`\n",
    "- `Girth` (diameter, which is twice $r$)\n",
    "- `Height`\n",
    "\n",
    "In other words, we basically have everything in the formula.\n",
    "Let's create a new column that is closer to what we want, `Girth` * `Girth` * `Height`:\n",
    "\n",
    "- Set `dataframe` to with `dataframe` do `assign` using\n",
    "    - freestyle `GGH=` *followed by* `dataframe [ \"Girth\" ] * dataframe [ \"Girth\" ] * dataframe [ \"Height\" ]`\n",
    "\n",
    "- `dataframe` (to display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, `GGH` is an interaction. \n",
    "Often when we use interactions in a model, we also include the variables that the interactions are made of (also known as **main effects**).\n",
    "However, in this case, that doesn't make sense because we know the interaction is close to the definition of `Volume`.\n",
    "\n",
    "So let's fit a new model using *just* `GGH`, save its predictions and residuals, and plot its predicted vs. residual diagnostic plot.\n",
    "\n",
    "First, fit the model:\n",
    "\n",
    "- with `lm` do `fit` using\n",
    "    - `dataframe [ ]` containing a list containing\n",
    "        - `\"GGH\"`\n",
    "    - `dataframe [ ]` containing a list containing\n",
    "        - `\"Volume\"` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics 2\n",
    "\n",
    "Save the predictions:\n",
    "\n",
    "- Set `dataframe` to with `dataframe` do `assign` using\n",
    "    - freestyle `predictions2=` *followed by*\n",
    "        - with `lm` do `predict` using\n",
    "            - `dataframe [ ]` containing a list containing\n",
    "                - `\"GGH\"`\n",
    "- `dataframe` (to display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the residuals:\n",
    "    \n",
    "- Set `dataframe` to with `dataframe` do `assign` using\n",
    "    - freestyle `residuals2=` *followed by* `dataframe [ \"Volume\" ] - dataframe [ \"predictions2\" ]`\n",
    "\n",
    "- `dataframe` (to display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plot the predicted vs residuals to check linearity and equal variance:\n",
    "\n",
    "- with `px` do `scatter` using\n",
    "    - `dataframe`\n",
    "    - freestyle `x=\"predictions2\"`\n",
    "    - freestyle `y=\"residuals2\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good plot.\n",
    "Most of the residuals are close to zero, and what residuals aren't are fairly evenly spread.\n",
    "We want to see an evenly spaced band above and below 0 as we scan from left to right, and we do.\n",
    "\n",
    "With this new model, calculate $r^2$ by copying the blocks above and making appropriate adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpython",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
